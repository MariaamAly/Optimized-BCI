{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "GitHub - code referenced from  https://github.com/stianyu/BCI_Competition_III_IVa.git\n",
    "\n",
    "mne documentation for CSP filtering and LDA implementation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\n",
    "\n",
    "GitHub issue raised for not having montage as the plot_patterns are not working\n",
    "https://github.com/mne-tools/mne-python/issues/4835\n",
    "\n",
    "Dataset description   http://www.bbci.de/competition/iv/desc_1.html\n",
    "\n",
    "In this dataset file c, d, e have the artificial data and a, b, f, g have the original data so the results can be different\n",
    "files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter,filtfilt\n",
    "import scipy.signal as sg\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from scipy.fftpack import fft, ifft\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# ! pip install -q scikit-plot\n",
    "import scikitplot as skplt\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_evaluation = pd.DataFrame({'Model': [],\n",
    "                           'Details':[],\n",
    "                           'RMSE(train)':[],\n",
    "                           'R-squared (train)':[],\n",
    "                           'Adj R-squared (train)':[],\n",
    "                           'MAE (train)':[],\n",
    "                           'RMSE (test)':[],\n",
    "                           'R-squared (test)':[],\n",
    "                           'Adj R-squared (test)':[],\n",
    "                           'MAE(test)':[],\n",
    "                           '10-Fold Cross Validation':[]})\n",
    "\n",
    "reg_evaluation2 = pd.DataFrame({'Model': [],\n",
    "                           'Test':[],\n",
    "                           '1':[],\n",
    "                           '2':[],\n",
    "                           '3':[],\n",
    "                           '4':[],\n",
    "                           '5':[],\n",
    "                           '6':[],\n",
    "                           '7':[],\n",
    "                           '8':[],\n",
    "                           '9':[],\n",
    "                           '10':[],\n",
    "                           'Mean':[]})\n",
    "\n",
    "classification_evaluation = pd.DataFrame({'Model': [],\n",
    "                           'Accuracy(train)':[],\n",
    "                           'Precision(train)':[],\n",
    "                           'Recall(train)':[],\n",
    "                           'F1_score(train)':[],\n",
    "                           'Accuracy(test)':[],\n",
    "                           'Precision(test)':[],\n",
    "                           'Recalll(test)':[],\n",
    "                           'F1_score(test)':[]})\n",
    "\n",
    "classification_evaluation2 = pd.DataFrame({'Model': [],\n",
    "                           'Test':[],\n",
    "                           '1':[],\n",
    "                           '2':[],\n",
    "                           '3':[],\n",
    "                           '4':[],\n",
    "                           '5':[],\n",
    "                           '6':[],\n",
    "                           '7':[],\n",
    "                           '8':[],\n",
    "                           '9':[],\n",
    "                           '10':[],\n",
    "                           'Mean':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1a_1000Hz.mat'\n",
    "F1 = loadmat(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'cnt', 'mrk', 'nfo'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -97, -100, -133, ...,  -26,   74,   28],\n",
       "       [ -32,  -70,  -34, ...,   37,  136,   84],\n",
       "       [   8,   -7,   19, ...,  110,  203,  162],\n",
       "       ...,\n",
       "       [2500, 2590, 2463, ..., 1364, 1850, 1197],\n",
       "       [2422, 2528, 2338, ..., 1311, 1790, 1148],\n",
       "       [2364, 2469, 2261, ..., 1248, 1714, 1106]], dtype=int16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = {}\n",
    "F1['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "td['data'] = F1['cnt'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[1000]], dtype=uint16)]], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['nfo']['fs'][True][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[array(['left'], dtype='<U4'), array(['foot'], dtype='<U4')]],\n",
       "              dtype=object)                                                  ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['nfo']['classes'][True][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[array(['AF3'], dtype='<U3'), array(['AF4'], dtype='<U3'),\n",
       "                array(['F5'], dtype='<U2'), array(['F3'], dtype='<U2'),\n",
       "                array(['F1'], dtype='<U2'), array(['Fz'], dtype='<U2'),\n",
       "                array(['F2'], dtype='<U2'), array(['F4'], dtype='<U2'),\n",
       "                array(['F6'], dtype='<U2'), array(['FC5'], dtype='<U3'),\n",
       "                array(['FC3'], dtype='<U3'), array(['FC1'], dtype='<U3'),\n",
       "                array(['FCz'], dtype='<U3'), array(['FC2'], dtype='<U3'),\n",
       "                array(['FC4'], dtype='<U3'), array(['FC6'], dtype='<U3'),\n",
       "                array(['CFC7'], dtype='<U4'), array(['CFC5'], dtype='<U4'),\n",
       "                array(['CFC3'], dtype='<U4'), array(['CFC1'], dtype='<U4'),\n",
       "                array(['CFC2'], dtype='<U4'), array(['CFC4'], dtype='<U4'),\n",
       "                array(['CFC6'], dtype='<U4'), array(['CFC8'], dtype='<U4'),\n",
       "                array(['T7'], dtype='<U2'), array(['C5'], dtype='<U2'),\n",
       "                array(['C3'], dtype='<U2'), array(['C1'], dtype='<U2'),\n",
       "                array(['Cz'], dtype='<U2'), array(['C2'], dtype='<U2'),\n",
       "                array(['C4'], dtype='<U2'), array(['C6'], dtype='<U2'),\n",
       "                array(['T8'], dtype='<U2'), array(['CCP7'], dtype='<U4'),\n",
       "                array(['CCP5'], dtype='<U4'), array(['CCP3'], dtype='<U4'),\n",
       "                array(['CCP1'], dtype='<U4'), array(['CCP2'], dtype='<U4'),\n",
       "                array(['CCP4'], dtype='<U4'), array(['CCP6'], dtype='<U4'),\n",
       "                array(['CCP8'], dtype='<U4'), array(['CP5'], dtype='<U3'),\n",
       "                array(['CP3'], dtype='<U3'), array(['CP1'], dtype='<U3'),\n",
       "                array(['CPz'], dtype='<U3'), array(['CP2'], dtype='<U3'),\n",
       "                array(['CP4'], dtype='<U3'), array(['CP6'], dtype='<U3'),\n",
       "                array(['P5'], dtype='<U2'), array(['P3'], dtype='<U2'),\n",
       "                array(['P1'], dtype='<U2'), array(['Pz'], dtype='<U2'),\n",
       "                array(['P2'], dtype='<U2'), array(['P4'], dtype='<U2'),\n",
       "                array(['P6'], dtype='<U2'), array(['PO1'], dtype='<U3'),\n",
       "                array(['PO2'], dtype='<U3'), array(['O1'], dtype='<U2'),\n",
       "                array(['O2'], dtype='<U2')]], dtype=object)                ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['nfo']['clab'][True][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[-0.20109028],\n",
       "               [ 0.20109028],\n",
       "               [-0.48547489],\n",
       "               [-0.32894737],\n",
       "               [-0.16535231],\n",
       "               [ 0.        ],\n",
       "               [ 0.16535231],\n",
       "               [ 0.32894737],\n",
       "               [ 0.48547489],\n",
       "               [-0.60591541],\n",
       "               [-0.39919579],\n",
       "               [-0.19765935],\n",
       "               [ 0.        ],\n",
       "               [ 0.19765935],\n",
       "               [ 0.39919579],\n",
       "               [ 0.60591541],\n",
       "               [-0.74834683],\n",
       "               [-0.52472976],\n",
       "               [-0.30963911],\n",
       "               [-0.10226303],\n",
       "               [ 0.10226303],\n",
       "               [ 0.30963911],\n",
       "               [ 0.52472976],\n",
       "               [ 0.74834683],\n",
       "               [-0.87719298],\n",
       "               [-0.64569058],\n",
       "               [-0.421549  ],\n",
       "               [-0.20773757],\n",
       "               [ 0.        ],\n",
       "               [ 0.20773757],\n",
       "               [ 0.421549  ],\n",
       "               [ 0.64569058],\n",
       "               [ 0.87719298],\n",
       "               [-0.74834683],\n",
       "               [-0.52472976],\n",
       "               [-0.30963911],\n",
       "               [-0.10226303],\n",
       "               [ 0.10226303],\n",
       "               [ 0.30963911],\n",
       "               [ 0.52472976],\n",
       "               [ 0.74834683],\n",
       "               [-0.60591541],\n",
       "               [-0.39919579],\n",
       "               [-0.19765935],\n",
       "               [ 0.        ],\n",
       "               [ 0.19765935],\n",
       "               [ 0.39919579],\n",
       "               [ 0.60591541],\n",
       "               [-0.48547489],\n",
       "               [-0.32894737],\n",
       "               [-0.16535231],\n",
       "               [ 0.        ],\n",
       "               [ 0.16535231],\n",
       "               [ 0.32894737],\n",
       "               [ 0.48547489],\n",
       "               [-0.10395865],\n",
       "               [ 0.10395865],\n",
       "               [-0.17113186],\n",
       "               [ 0.17113186]])]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['nfo']['xpos'][True][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[ 0.68656518],\n",
       "               [ 0.68656518],\n",
       "               [ 0.52547424],\n",
       "               [ 0.46520183],\n",
       "               [ 0.43208641],\n",
       "               [ 0.421549  ],\n",
       "               [ 0.43208641],\n",
       "               [ 0.46520183],\n",
       "               [ 0.52547424],\n",
       "               [ 0.27165704],\n",
       "               [ 0.23384348],\n",
       "               [ 0.21394494],\n",
       "               [ 0.20773757],\n",
       "               [ 0.21394494],\n",
       "               [ 0.23384348],\n",
       "               [ 0.27165704],\n",
       "               [ 0.15177169],\n",
       "               [ 0.12553103],\n",
       "               [ 0.11086096],\n",
       "               [ 0.10426648],\n",
       "               [ 0.10426648],\n",
       "               [ 0.11086096],\n",
       "               [ 0.12553103],\n",
       "               [ 0.15177169],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [ 0.        ],\n",
       "               [-0.15177169],\n",
       "               [-0.12553103],\n",
       "               [-0.11086096],\n",
       "               [-0.10426648],\n",
       "               [-0.10426648],\n",
       "               [-0.11086096],\n",
       "               [-0.12553103],\n",
       "               [-0.15177169],\n",
       "               [-0.27165704],\n",
       "               [-0.23384348],\n",
       "               [-0.21394494],\n",
       "               [-0.20773757],\n",
       "               [-0.21394494],\n",
       "               [-0.23384348],\n",
       "               [-0.27165704],\n",
       "               [-0.52547424],\n",
       "               [-0.46520183],\n",
       "               [-0.43208641],\n",
       "               [-0.421549  ],\n",
       "               [-0.43208641],\n",
       "               [-0.46520183],\n",
       "               [-0.52547424],\n",
       "               [-0.65583812],\n",
       "               [-0.65583812],\n",
       "               [-0.86033797],\n",
       "               [-0.86033797]])]], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['nfo']['ypos'][True][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[  20913,   28913,   36913,   44913,   52914,   60914,   68914,\n",
       "                  76915,   84915,   92915,  100915,  108916,  116916,  124916,\n",
       "                 132917,  162940,  170941,  178941,  186941,  194942,  202942,\n",
       "                 210942,  218942,  226943,  234943,  242943,  250944,  258944,\n",
       "                 266944,  274944,  304948,  312949,  320949,  328949,  336950,\n",
       "                 344950,  352950,  360950,  368951,  376951,  384951,  392952,\n",
       "                 400952,  408952,  416952,  446957,  454958,  462958,  470958,\n",
       "                 478959,  486959,  494959,  502959,  510960,  518960,  526960,\n",
       "                 534961,  542961,  550961,  558961,  588950,  596951,  604951,\n",
       "                 612951,  620952,  628952,  636952,  644952,  652953,  660953,\n",
       "                 668953,  676953,  684954,  692954,  700954,  730944,  738944,\n",
       "                 746944,  754945,  762945,  770945,  778945,  786946,  794946,\n",
       "                 802946,  810947,  818947,  826947,  834947,  842948,  872937,\n",
       "                 880937,  888937,  896938,  904938,  912938,  920938,  928939,\n",
       "                 936939,  944939,  973846,  981846,  989847,  997847, 1005847,\n",
       "                1013848, 1021848, 1029848, 1037848, 1045849, 1053849, 1061849,\n",
       "                1069849, 1077850, 1085850, 1115842, 1123842, 1131843, 1139843,\n",
       "                1147843, 1155844, 1163844, 1171844, 1179844, 1187845, 1195845,\n",
       "                1203845, 1211846, 1219846, 1227846, 1257866, 1265866, 1273866,\n",
       "                1281867, 1289867, 1297867, 1305868, 1313868, 1321868, 1329868,\n",
       "                1337869, 1345869, 1353869, 1361870, 1369870, 1399859, 1407860,\n",
       "                1415860, 1423860, 1431861, 1439861, 1447861, 1455861, 1463862,\n",
       "                1471862, 1479862, 1487862, 1495863, 1503863, 1511863, 1541852,\n",
       "                1549853, 1557853, 1565853, 1573854, 1581854, 1589854, 1597854,\n",
       "                1605855, 1613855, 1621855, 1629855, 1637856, 1645856, 1653856,\n",
       "                1683846, 1691846, 1699846, 1707847, 1715847, 1723847, 1731847,\n",
       "                1739848, 1747848, 1755848, 1763849, 1771849, 1779849, 1787849,\n",
       "                1795850, 1825839, 1833839, 1841839, 1849840, 1857840, 1865840,\n",
       "                1873840, 1881841, 1889841, 1897841]], dtype=int32)            ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1['mrk']['pos'][True][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(F1['mrk']['y'][True][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(F1['nfo']['classes'][True][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "fp = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1a_1000Hz.mat'\n",
    "\n",
    "mat = sio.loadmat(fp, squeeze_me=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat['mrk']['y'][True][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "t['data'] = mat['cnt'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat['cnt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['freq'] = mat['nfo']['fs'][True][0]\n",
    "t['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['ch_names'] = mat['nfo']['clab'][True][0]\n",
    "t['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['elctrode_x'] = mat['nfo']['xpos'][True][0]\n",
    "t['elctrode_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['electrode_y'] = mat['nfo']['ypos'][True][0]\n",
    "t['electrode_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['cue'] = mat['mrk']['pos'][True][0] \n",
    "t['cue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# t['labels'] = np.nan_to_num(mat['mrk']['y'][True][0]).astype(int)\n",
    "t['labels'] = mat['mrk']['y'][True][0]\n",
    "t['labels'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(t['labels'] == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(t['labels'] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_trials = np.where(t['labels'] == True)[0]\n",
    "testing_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['classes'] = F1['nfo']['classes'][True][0]\n",
    "t['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat['nfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['n_trials'] = np.where(t['labels'] == 1)[0]\n",
    "t['n_trials'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array(t)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessors\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.decoding import CSP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inputmat(fp):\n",
    "    \"\"\"load .mat file and return m as a dict\"\"\"\n",
    "    mat = sio.loadmat(fp, squeeze_me=True)\n",
    "    td = {}  # create a dict\n",
    "\n",
    "    # Numpy array of size channel_num * points.\n",
    "    td['data'] = mat['cnt'].T  # data\n",
    "    td['freq'] = mat['nfo']['fs'][True][0]  # Sampling frequency\n",
    "\n",
    "    # channel names are necessary information for creating a rawArray.\n",
    "    td['ch_names'] = mat['nfo']['clab'][True][0]\n",
    "\n",
    "    # Position of channels\n",
    "    td['electrode_x'] = mat['nfo']['xpos'][True][0]\n",
    "    td['electrode_y'] = mat['nfo']['ypos'][True][0]\n",
    "\n",
    "    #make trials by finding trials and its data\n",
    "    td['cue'] = mat['mrk']['pos'][True][0] #time of cue\n",
    "    td['labels'] = mat['mrk']['y'][True][0] #labels of the data\n",
    "    return td\n",
    "\n",
    "\n",
    "\"\"\" do we really need number of trials and also what is the meaning of the line in \n",
    "events[:, 0] and events[:, 2]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def creatEventsArray(fp):\n",
    "    \"\"\"Create events array. The second column default to zero.\"\"\"\n",
    "    td = inputmat(fp)\n",
    "    events = np.zeros((td['labels'].size, 3), int) #here we have made a matrix type array of the size of label.size*3\n",
    "#     print(events)\n",
    "    events[:, 0] = td['cue']  # The first column is the sample number of the event.\n",
    "#     print(events[:, 0])\n",
    "    events[:, 2] = td['labels']  # The third column is the new event value.\n",
    "#     print(events[:, 2])\n",
    "    return events, td['labels']\n",
    "\n",
    "\n",
    "def creatRawArray(fp):\n",
    "    \"\"\"Create a mne.io.RawArray object, data: array, shape (n_channels, n_times)\"\"\"\n",
    "    td = inputmat(fp)\n",
    "    ch_names = td['ch_names'].tolist()\n",
    "    info = mne.create_info(ch_names, td['freq'], 'eeg')  # Create info for raw\n",
    "    raw = mne.io.RawArray(td['data'], info, first_samp=0, copy='auto', verbose=None)\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from mne.decoding import CSP\n",
    "from mne.channels import read_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The files c, d, e are the artificial data so we can test out our labels in different ways in them\"\"\"\n",
    "\n",
    "\n",
    "file1 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1a_1000Hz.mat'\n",
    "file2 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1b_1000Hz.mat'\n",
    "file3 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1c_1000Hz.mat'\n",
    "file4 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1d_1000Hz.mat'\n",
    "file5 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1e_1000Hz.mat'\n",
    "file6 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1f_1000Hz.mat'\n",
    "file7 = r'Data/BCICIV_1calib_1000Hz_mat/BCICIV_calib_ds1g_1000Hz.mat'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"making the file dictionary and channel dictionary for easy pickup\"\"\"\n",
    "fp = {\n",
    "    'ds1a': file1,\n",
    "    'ds1b': file2,\n",
    "#     'ds1c': file3, #these three are artificial datatsets\n",
    "#     'ds1d': file4,\n",
    "#     'ds1e': file5,\n",
    "    'ds1f': file6,\n",
    "    'ds1g': file7,\n",
    "}\n",
    "\n",
    "# pick_chan = {\n",
    "#     'ds1a': ['C3', 'Cz', 'C5'],\n",
    "#     'ds1b': ['C3', 'Cz', 'C5'],\n",
    "#     'ds1c': ['C3', 'Cz', 'C5'],  \n",
    "#     'ds1d': ['C3', 'Cz', 'C5'],\n",
    "#     'ds1e': ['C3', 'Cz', 'C5'],\n",
    "#     'ds1f': ['C3', 'Cz', 'C5'],\n",
    "#     'ds1g': ['C3', 'Cz', 'C5'],\n",
    "# }\n",
    "\n",
    "low_freq, high_freq = 7., 30.\n",
    "tmin, tmax = 0., 3.5\n",
    "\n",
    "# event_id\n",
    "# event_id = {'right': 1, 'foot': 2}\n",
    "#changing eventid from right to left\n",
    "event_id = {'left': -1, 'foot': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=59, n_times=1905940\n",
      "    Range : 0 ... 1905939 =      0.000 ...  1905.939 secs\n",
      "Ready.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 1651 samples (1.651 sec)\n",
      "\n",
      "(200, 59, 1001)\n",
      "ShuffleSplit(n_splits=5, random_state=42, test_size=0.2, train_size=None)\n",
      "<generator object BaseShuffleSplit.split at 0x00000216890C8D60>\n"
     ]
    }
   ],
   "source": [
    "raw = creatRawArray(file1)\n",
    "events, labels = creatEventsArray(file1)\n",
    "    \n",
    "# strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "\n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "\n",
    "\n",
    "# Apply band-pass filter\n",
    "raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "\n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "# event_train = eventsTrain(fp[f])\n",
    "epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                    verbose=False)#, picks=picks)\n",
    "\n",
    "epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "labels = epochs.events[:, -1] - 2\n",
    "\n",
    "# Define a monte-carlo cross-validation generator (reduce variance):\n",
    "scores = []\n",
    "epochs_data = epochs.get_data()\n",
    "epochs_data_train = epochs_train.get_data()\n",
    "\n",
    "#splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(epochs_data_train, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "print(epochs_data_train.shape)\n",
    "print(cv)\n",
    "print(cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustedR2(r2,n,k=3):\n",
    "    return r2-(k-1)/(n-k)*(1-r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.3e+03 (2.2e-16 eps * 59 dim * 1.8e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.2e+03 (2.2e-16 eps * 59 dim * 1.7e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.1e+03 (2.2e-16 eps * 59 dim * 1.6e+17  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "[-3 -1 -3 -1 -1 -1 -3 -1 -3 -1 -3 -3 -3 -1 -1 -1 -1 -1 -1 -1 -3 -1 -3 -3\n",
      " -1 -3 -1 -3 -3 -1 -3 -3 -1 -3 -3 -3 -3 -1 -1 -1]\n",
      "[-3 -1 -1 -3 -3 -3 -3 -3 -3 -1 -3 -1 -1 -1 -3 -3 -3 -3 -1 -1 -3 -1 -3 -3\n",
      " -3 -1 -1 -3 -3 -1 -1 -1 -1 -3 -1 -3 -3 -3 -3 -3 -3 -3 -1 -3 -3 -3 -3 -3\n",
      " -1 -3 -3 -1 -1 -3 -1 -3 -3 -1 -1 -1 -1 -3 -3 -3 -3 -1 -3 -3 -3 -3 -3 -1\n",
      " -1 -1 -3 -1 -1 -1 -3 -3 -3 -1 -1 -1 -3 -3 -3 -1 -3 -1 -3 -1 -1 -3 -3 -3\n",
      " -1 -1 -3 -1 -1 -3 -3 -1 -3 -3 -3 -1 -1 -1 -3 -3 -3 -3 -3 -1 -1 -1 -3 -1\n",
      " -3 -1 -3 -3 -3 -1 -3 -1 -3 -1 -1 -1 -1 -1 -1 -3 -3 -1 -1 -1 -1 -1 -1 -1\n",
      " -3 -1 -1 -1 -1 -3 -1 -1 -1 -1 -3 -1 -3 -1 -1 -1]\n",
      "Classification accuracy: nan / Chance level: 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Continuum\\anaconda3\\envs\\mne\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "C:\\Users\\rahul\\AppData\\Local\\Continuum\\anaconda3\\envs\\mne\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Assemble a classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "features = 3\n",
    "pred = clf.predict(X_test)\n",
    "rmse_train = float(format(np.sqrt(metrics.mean_squared_error(clf.predict(X_train), y_train)),'.3f'))\n",
    "r2_train = float(format(clf.score(X_train, y_train),'.3f'))\n",
    "# ar2_train = float(format(adjustedR2(clf.score(X_train, y_train),X_train.shape[0],3),'.3f'))\n",
    "mae_train=float(format((metrics.mean_absolute_error(clf.predict(X_train), y_train)),'.3f'))\n",
    "\n",
    "rmse_test = float(format(np.sqrt(metrics.mean_squared_error(clf.predict(X_test), y_test)),'.3f'))\n",
    "r2_test = float(format(clf.score(X_test, y_test),'.3f'))\n",
    "# ar2_test = float(format(adjustedR2(clf.score(X_test, y_test),X_test.shape[0],3),'.3f'))\n",
    "mae_test=float(format((metrics.mean_absolute_error(clf.predict(X_test), y_test)),'.3f'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "cv = float(format(cross_val_score(clf,X_train, y_train,cv=10).mean(),'.3f'))\n",
    "r = reg_evaluation.shape[0]\n",
    "reg_evaluation.loc[r] = ['SVR','All features',rmse_train,r2_train,ar2_train,mae_train,rmse_test,r2_test,ar2_test,mae_test,cv]\n",
    "\n",
    "\n",
    "# # Print the predicted and actual value for the test set\n",
    "# MLR_y_test_prediction= clf.predict(X_test)\n",
    "# np.savetxt('SVR_test_prediction.csv', MLR_y_test_prediction, delimiter=',', fmt='%s')\n",
    "# np.savetxt('SVR_test_actual.csv', y_test, delimiter=',', fmt='%s')\n",
    "\n",
    "# # Print the predicted and actual value for the traing set\n",
    "# MLR_y_train_prediction= clf.predict(X_train)\n",
    "# np.savetxt('SVR_train_prediction.csv', MLR_y_train_prediction, delimiter=',', fmt='%s')\n",
    "# np.savetxt('SVR_train_actual.csv', y_train, delimiter=',', fmt='%s')\n",
    "\n",
    "# X_standardized = scaler.transform(X)\n",
    "# MLR_y_pred_entire_data = clf.predict(X_standardized)\n",
    "# np.savetxt('SVR_entire_prediction.csv', MLR_y_pred_entire_data, delimiter=',', fmt='%s')\n",
    "# np.savetxt('SVR_entire_actual.csv', y, delimiter=',', fmt='%s')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pred, y_test,  'ro')\n",
    "# plt.ylabel('Predicted data')\n",
    "# plt.xlabel('Actual data')\n",
    "# plt.show()\n",
    "\n",
    "# print(MLR_y_test_prediction)\n",
    "# print(MLR_y_train_prediction)\n",
    "# print(intact)\n",
    "\n",
    "\n",
    "# Printing the results\n",
    "class_balance = np.mean(labels == labels[0])\n",
    "class_balance = max(class_balance, 1. - class_balance)\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "# plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code from the mne documentation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\"\"\"\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code from the mne documentation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\"\"\"\n",
    "\n",
    "#this is the SVM classifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mne.decoding import Scaler\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    SVM_C = svm.SVC(kernel='rbf', degree=100)\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('SVM_C', SVM_C)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code from the mne documentation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\"\"\"\n",
    "\n",
    "#this is the Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    RFC = RandomForestClassifier()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('RFC', RFC)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code from the mne documentation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\"\"\"\n",
    "\n",
    "#this is the MLP neural net classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    MLP_C = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(16, 16), random_state=42)\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('MLP_C', MLP_C)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code from the mne documentation\n",
    "https://mne.tools/dev/auto_examples/decoding/plot_decoding_csp_eeg.html\"\"\"\n",
    "\n",
    "#this is the Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    RFC = RandomForestClassifier()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('RFC', RFC)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "https://www.researchgate.net/publication/321726030_The_Performance_Analysis_of_K-Nearest_Neighbors_K-NN_Algorithm_for_Motor_Imagery_Classification_Based_on_EEG_Signal\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Knearest neighbour classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    KNN_C = KNeighborsClassifier(n_neighbors=15, algorithm='auto', n_jobs=1, metric='chebyshev')\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('KNN_C', KNN_C)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    DTC = DecisionTreeClassifier(random_state=42, criterion='gini')\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('DTC', DTC)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    LLR = LogisticRegression(random_state=42, penalty='l2', solver='lbfgs', verbose=0, n_jobs=1)\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('LLR', LLR)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    GNB = GaussianNB()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('GNB', GNB)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Complement Naive Bayes\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    CNB = ComplementNB()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('CNB', CNB)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    BNB = BernoulliNB()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('BNB', BNB)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Categorical Naive Bayes\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    catNB = CategoricalNB()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('catNB', catNB)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code reference from the mne documentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#this is the Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "for f in fp:\n",
    "    raw = creatRawArray(fp[f])\n",
    "    events, labels = creatEventsArray(fp[f])\n",
    "    \n",
    "    # strip channel names of \".\" characters\n",
    "#     raw.rename_channels(lambda x: x.strip('.'))\n",
    "    \n",
    "#     print(raw.info)\n",
    "#     print(raw.ch_names)\n",
    "#     #adding montage\n",
    "#     montage = make_standard_montage('standard_1020')\n",
    "#     raw.set_montage(montage)\n",
    "    \n",
    "    \n",
    "    # Apply band-pass filter\n",
    "    raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
    "    \n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "\n",
    "    # event_train = eventsTrain(fp[f])\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
    "                        verbose=False)#, picks=picks)\n",
    "\n",
    "    epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "    labels = epochs.events[:, -1] - 2\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    epochs_data = epochs.get_data()\n",
    "    epochs_data_train = epochs_train.get_data()\n",
    "    \n",
    "    #shuffling the data\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(epochs_data_train)\n",
    "\n",
    "    # Assemble a classifier\n",
    "    MNB = MultinomialNB()\n",
    "    csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
    "\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    clf = Pipeline([('CSP', csp), ('MNB', MNB)])\n",
    "    scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores), class_balance))\n",
    "\n",
    "    # plot CSP patterns estimated on full data for visualization\n",
    "#     csp.fit_transform(epochs_data, labels)\n",
    "\n",
    "#     csp.plot_patterns(epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
